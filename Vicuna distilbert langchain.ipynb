{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a933ecd-5d4c-461c-bcd3-f46e08f6400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /gpfs/space/projects/stud_ml_22/NLP/data/chroma_distilbert/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035e9ace-a01e-4d35-96be-019cbbd19e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_exploration.py  main.py  requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /gpfs/space/projects/stud_ml_22/NLP/llama_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805432c1-8cc5-42c2-8da6-3c95473976f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/space/home/dzvenymy/.conda/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModel\n",
    "from accelerate import Accelerator\n",
    "import time\n",
    "\n",
    "from pydantic import BaseModel, Extra, Field, root_validator\n",
    "from typing import Any, List, Optional, Dict, Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f060a4-c67b-48a1-bc73-7f57353cfdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cecf3a8-2fff-4dc3-910f-6db44ccea08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_dir = '/gpfs/space/projects/stud_ml_22/NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96371d2e-ea5e-420a-85e8-453a228b9181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/en/latest/getting_started/getting_started.html\n",
    "# https://python.langchain.com/en/latest/use_cases/question_answering.html\n",
    "# https://python.langchain.com/en/latest/modules/indexes/getting_started.html\n",
    "# https://python.langchain.com/en/latest/use_cases/question_answering/semantic-search-over-chat.html\n",
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.schema import Generation\n",
    "from langchain.schema import PromptValue, LLMResult\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# langchain.document_loaders.DataFrameLoader has a quite a limited functionality\n",
    "class DataFrameLoader(BaseLoader):\n",
    "    def __init__(self, data_frame: Any, page_content_columns: List[str]):\n",
    "        if not isinstance(data_frame, pd.DataFrame):\n",
    "            raise ValueError(\n",
    "                f\"Expected data_frame to be a pd.DataFrame, got {type(data_frame)}\"\n",
    "            )\n",
    "        self.data_frame = data_frame\n",
    "        self.page_content_columns = page_content_columns\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        result = []\n",
    "        for i, row in self.data_frame.iterrows():\n",
    "            text = \"\"\n",
    "            metadata = {}\n",
    "            for col in self.page_content_columns:\n",
    "                data = row[col]\n",
    "                if isinstance(data, list):\n",
    "                    text += \"\".join(data) + \"\\n\"\n",
    "                elif isinstance(data, str):\n",
    "                    text += data + \"\\n\"\n",
    "                else:\n",
    "                    print(f\"[IGNORED] [{i}] [{col}] {data}\")\n",
    "\n",
    "            metadata_temp = row.to_dict()\n",
    "            for col in self.page_content_columns:\n",
    "                metadata_temp.pop(col)\n",
    "            # Metadata is a dict where a value can only be str, int, or float. Delete other types.\n",
    "            for key, value in metadata_temp.items():\n",
    "                if isinstance(value, (str, int, float)):\n",
    "                    metadata[key] = value\n",
    "\n",
    "            result.append(Document(page_content=text, metadata=metadata))\n",
    "        return result\n",
    "\n",
    "\n",
    "# class MyLanguageModel(BaseLanguageModel):\n",
    "#     def generate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "#                         callbacks: Callbacks = None) -> LLMResult:\n",
    "#         generation = Generation(text=\"Hello World!\")\n",
    "#         result = LLMResult(generations=[[generation]])\n",
    "#         return result\n",
    "\n",
    "#     async def agenerate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "#                                callbacks: Callbacks = None) -> LLMResult:\n",
    "#         pass  # \"whatever dude\"\n",
    "\n",
    "\n",
    "# # NOTE: the OpenAIEmbeddings embeddings have the dimensionality of 1536\n",
    "# class MyEmbeddings(Embeddings):\n",
    "#     def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "#         return [[1.0] * 1536, [2.0] * 1536]\n",
    "\n",
    "#     def embed_query(self, text: str) -> List[float]:\n",
    "#         return [1.0] * 1536\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775d3f22-b482-476a-bf6a-fd9a43292e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(encoder_only = False):\n",
    "        PATH_TO_CONVERTED_WEIGHTS = os.path.join(\n",
    "            shared_dir, \"llama/7B_Vicuna_added/\")\n",
    "\n",
    "        device = torch.device(\n",
    "            \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        config = AutoConfig.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
    "        config.max_position_embeddings = 1024\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            PATH_TO_CONVERTED_WEIGHTS,\n",
    "            config=config,\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=not args.no_gradient_checkpointing,\n",
    "            load_in_8bit=True,\n",
    "            device_map={\"\": Accelerator().process_index},\n",
    "            # device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da61bb46-b496-4482-8439-682aa8302ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseMessage, LLMResult, PromptValue, get_buffer_string\n",
    "\n",
    "class LlamaWrapperModel(BaseLanguageModel):\n",
    "    model: Any\n",
    "    \n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        values['model'] = get_model()\n",
    "        # values['model'] = 'model'\n",
    "        return values\n",
    "    \n",
    "    def predict(self, text: str, *, stop: Optional[Sequence[str]] = None) -> str:\n",
    "        pass\n",
    "    \n",
    "    def predict_messages(\n",
    "        self, messages: List[BaseMessage], *, stop: Optional[Sequence[str]] = None\n",
    "    ) -> BaseMessage:\n",
    "        pass\n",
    "\n",
    "    def generate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                        callbacks: Callbacks = None) -> LLMResult:\n",
    "\n",
    "        PATH_TO_CONVERTED_TOKENIZER = os.path.join(\n",
    "            shared_dir, \"llama/7B_converted/\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
    "              \n",
    "        with torch.no_grad():\n",
    "            prompt = prompts[0].text\n",
    "            \n",
    "            print(\"Tokenizing...\")\n",
    "            s = time.time()\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            e1 = time.time()\n",
    "            print(\"Time to tokenize: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e1 - s)))\n",
    "            \n",
    "            print(\"Generating...\")\n",
    "            generate_ids = self.model.generate(input_ids=inputs.input_ids.to(\n",
    "                device), max_length=5000)  # max_length = max_new_tokens + prompt_length\n",
    "            e2 = time.time()\n",
    "            print(\"Time to generate: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e2 - e1)))\n",
    "            \n",
    "            print(\"Decoding...\")\n",
    "            text_result = tokenizer.batch_decode(\n",
    "                generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            e3 = time.time()\n",
    "            print(\"Time to decode: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e3 - e2)))\n",
    "\n",
    "        generation = Generation(text=text_result)\n",
    "        result = LLMResult(generations=[[generation]])\n",
    "        return result\n",
    "\n",
    "    async def agenerate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                               callbacks: Callbacks = None) -> LLMResult:\n",
    "        pass  # \"whatever dude\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fe38752-36ad-4250-a687-620573c315dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the OpenAIEmbeddings embeddings have the dimensionality of 1536\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class DistilbertEmbeddings(Embeddings):\n",
    "   \n",
    "    def __init__(self):       \n",
    "        self.model = AutoModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "       \n",
    "\n",
    "        # return values\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        # Tokenize the messages and generate embeddings\n",
    "        tokenized = [self.tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True) for text in texts]\n",
    "        padded = np.array([i + [0]*(512-len(i)) for i in tokenized])\n",
    "       \n",
    "        input_ids = torch.tensor(padded)\n",
    "        embds = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(input_ids)-32, 32):\n",
    "                batch = input_ids[i:i+32].to(device)\n",
    "                last_hidden_states = self.model(batch)[0][:,0,:].cpu().numpy().tolist()\n",
    "                embds.extend(last_hidden_states)\n",
    "            last_batch = input_ids[(len(input_ids) // 32) * 32:].to(device)\n",
    "            last_hidden_states = self.model(last_batch)[0][:,0,:].cpu().numpy().tolist()\n",
    "            embds.extend(last_hidden_states)\n",
    "            \n",
    "        return embds\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        # Tokenize the messages and generate embeddings\n",
    "        tokenized = self.tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "        padded = np.array([tokenized+ [0]*(512-len(tokenized))])\n",
    "        input_ids = torch.tensor(padded)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = self.model(input_ids.to(device))\n",
    "            last_hidden_states = last_hidden_states[0][:,0,:].cpu().numpy()\n",
    "        \n",
    "        return last_hidden_states[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07479bb1-d597-4159-b53e-a60b8d191a65",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81fe05e-37c0-4e67-aeae-9802845e57cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = pd.read_pickle(f'{shared_dir}/data/course_info.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b18015-4ffe-4d18-9361-92151a29edea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{shared_dir}/data/courses_info_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "749b3ab0-c7e7-4781-b17e-e2aaa654618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_bio = df[df['title_en'] == \"Bioinformatics Seminar\"]['all_course_info'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b69336-8f27-4782-b898-77633dcd853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "loader = DataFrameLoader(df, [\"title_en\"])\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "embeddings = DistilbertEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "880c83fd-09bc-4f44-a29e-ae3f3fad0a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IGNORED] [2853] [title_en] nan\n",
      "[IGNORED] [2857] [title_en] nan\n"
     ]
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78e7dc2c-f563-4a9a-9318-f9d29bc8324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: /gpfs/space/projects/stud_ml_22/NLP/data/chroma_distilbert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# this will create the chroma embedding database!!!\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=f\"{shared_dir}/data/chroma_distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac42fd64-a44f-4ae1-b691-9e560b2a9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.similarity_search_with_score(\"Special Seminar in Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25129f53-87d5-486f-a244-4c54553052fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = LlamaWrapperModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3599eb7-7d4d-4902-8412-dcc8a1f1e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\", \"all_course_info\"], template=\"{page_content}. {all_course_info}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b229b64f-4736-4eab-a559-06de223ad241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANNOT LOAD FROM DISK BECAUSE OF EMBD DIM INITIALIZATION BUG\n",
    "# db = Chroma(persist_directory=f\"{shared_dir}/data/chroma\", embedding_function=embeddings)  # load from disk \n",
    "retriever = db.as_retriever()\n",
    "chain_type_kwargs = {\"prompt\": llm_prompt}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs)\n",
    "qa.combine_documents_chain.document_prompt = chain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970ea21-5cc4-4852-85a5-d38b2a808d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the purpose of Bioinformatics Seminar?\"\n",
    "print(qa(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
