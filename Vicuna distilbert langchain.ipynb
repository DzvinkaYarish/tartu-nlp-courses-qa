{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a933ecd-5d4c-461c-bcd3-f46e08f6400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf /gpfs/space/projects/stud_ml_22/NLP/data/chroma_distilbert/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035e9ace-a01e-4d35-96be-019cbbd19e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_exploration.py  main.py  requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /gpfs/space/projects/stud_ml_22/NLP/llama_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3dc0af0-8b6a-48bf-9a6c-6d90d3c4a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!module load cuda/11.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a08636-8afc-4d58-9c88-481cd2532ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd ../bitsandbytes\n",
    "# !CUDA_VERSION=117 make cuda11x_nomatmul\n",
    "# !cd ../tartu-nlp-courses-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe30f5e-6704-4a0c-888e-fbe23ed52d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 22 01:38:53 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB            Off| 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   32C    P0               55W / 300W|      0MiB / 32768MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805432c1-8cc5-42c2-8da6-3c95473976f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModel\n",
    "from accelerate import Accelerator\n",
    "import time\n",
    "\n",
    "from pydantic import BaseModel, Extra, Field, root_validator\n",
    "from typing import Any, List, Optional, Dict, Sequence\n",
    "from chromadb.utils import embedding_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f060a4-c67b-48a1-bc73-7f57353cfdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cecf3a8-2fff-4dc3-910f-6db44ccea08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_dir = '/gpfs/space/projects/stud_ml_22/NLP'\n",
    "PATH_TO_CONVERTED_WEIGHTS = os.path.join(shared_dir, \"llama/13B_Vicuna_added/\")\n",
    "PATH_TO_CONVERTED_TOKENIZER = os.path.join(shared_dir, \"llama/13B_Vicuna_added/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96371d2e-ea5e-420a-85e8-453a228b9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.schema import Generation\n",
    "from langchain.schema import PromptValue, LLMResult\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# langchain.document_loaders.DataFrameLoader has a quite a limited functionality\n",
    "class DataFrameLoader(BaseLoader):\n",
    "    def __init__(self, data_frame: Any, page_content_columns: List[str]):\n",
    "        if not isinstance(data_frame, pd.DataFrame):\n",
    "            raise ValueError(\n",
    "                f\"Expected data_frame to be a pd.DataFrame, got {type(data_frame)}\"\n",
    "            )\n",
    "        self.data_frame = data_frame\n",
    "        self.page_content_columns = page_content_columns\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        result = []\n",
    "        for i, row in self.data_frame.iterrows():\n",
    "            text = \"\"\n",
    "            metadata = {}\n",
    "            for col in self.page_content_columns:\n",
    "                data = row[col]\n",
    "                if isinstance(data, list):\n",
    "                    text += \"\".join(data) + \"\\n\"\n",
    "                elif isinstance(data, str):\n",
    "                    text += data + \"\\n\"\n",
    "                else:\n",
    "                    print(f\"[IGNORED] [{i}] [{col}] {data}\")\n",
    "\n",
    "            metadata_temp = row.to_dict()\n",
    "            for col in self.page_content_columns:\n",
    "                metadata_temp.pop(col)\n",
    "            # Metadata is a dict where a value can only be str, int, or float. Delete other types.\n",
    "            for key, value in metadata_temp.items():\n",
    "                if isinstance(value, (str, int, float)):\n",
    "                    metadata[key] = value\n",
    "\n",
    "            result.append(Document(page_content=text, metadata=metadata))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "775d3f22-b482-476a-bf6a-fd9a43292e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(encoder_only = False):\n",
    "        device = torch.device(\n",
    "            \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        config = AutoConfig.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
    "        config.max_position_embeddings = 2048\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            PATH_TO_CONVERTED_WEIGHTS,\n",
    "            config=config,\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=not args.no_gradient_checkpointing,\n",
    "            load_in_8bit=True,\n",
    "            device_map={\"\": Accelerator().process_index},\n",
    "            # device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da61bb46-b496-4482-8439-682aa8302ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseMessage, LLMResult, PromptValue, get_buffer_string\n",
    "\n",
    "class LlamaWrapperModel(BaseLanguageModel):\n",
    "    model: Any\n",
    "    \n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        values['model'] = get_model()\n",
    "        # values['model'] = 'model'\n",
    "        return values\n",
    "    \n",
    "    def predict(self, text: str, *, stop: Optional[Sequence[str]] = None) -> str:\n",
    "        pass\n",
    "    \n",
    "    def predict_messages(\n",
    "        self, messages: List[BaseMessage], *, stop: Optional[Sequence[str]] = None\n",
    "    ) -> BaseMessage:\n",
    "        pass\n",
    "\n",
    "    def generate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                        callbacks: Callbacks = None) -> LLMResult:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
    "              \n",
    "        with torch.no_grad():\n",
    "            prompt = prompts[0].text\n",
    "            \n",
    "            print(\"Tokenizing...\")\n",
    "            s = time.time()\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            e1 = time.time()\n",
    "            print(\"Time to tokenize: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e1 - s)))\n",
    "            \n",
    "            print(\"Generating...\")\n",
    "            generate_ids = self.model.generate(input_ids=inputs.input_ids.to(\n",
    "                device), max_length=3000)  # max_length = max_new_tokens + prompt_length\n",
    "            e2 = time.time()\n",
    "            print(\"Time to generate: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e2 - e1)))\n",
    "            \n",
    "            print(\"Decoding...\")\n",
    "            text_result = tokenizer.batch_decode(\n",
    "                generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            e3 = time.time()\n",
    "            print(\"Time to decode: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e3 - e2)))\n",
    "\n",
    "        generation = Generation(text=text_result)\n",
    "        result = LLMResult(generations=[[generation]])\n",
    "        return result\n",
    "\n",
    "    async def agenerate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                               callbacks: Callbacks = None) -> LLMResult:\n",
    "        pass  # \"whatever dude\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fe38752-36ad-4250-a687-620573c315dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the OpenAIEmbeddings embeddings have the dimensionality of 1536\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class DistilbertEmbeddings(Embeddings):\n",
    "   \n",
    "    def __init__(self):       \n",
    "        self.model = AutoModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "       \n",
    "\n",
    "        # return values\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        # Tokenize the messages and generate embeddings\n",
    "        tokenized = [self.tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True) for text in texts]\n",
    "        padded = np.array([i + [0]*(512-len(i)) for i in tokenized])\n",
    "       \n",
    "        input_ids = torch.tensor(padded)\n",
    "        embds = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(input_ids)-32, 32):\n",
    "                batch = input_ids[i:i+32].to(device)\n",
    "                last_hidden_states = self.model(batch)[0][:,0,:].cpu().numpy().tolist()\n",
    "                embds.extend(last_hidden_states)\n",
    "            last_batch = input_ids[(len(input_ids) // 32) * 32:].to(device)\n",
    "            last_hidden_states = self.model(last_batch)[0][:,0,:].cpu().numpy().tolist()\n",
    "            embds.extend(last_hidden_states)\n",
    "            \n",
    "        return embds\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        # Tokenize the messages and generate embeddings\n",
    "        tokenized = self.tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "        padded = np.array([tokenized+ [0]*(512-len(tokenized))])\n",
    "        input_ids = torch.tensor(padded)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = self.model(input_ids.to(device))\n",
    "            last_hidden_states = last_hidden_states[0][:,0,:].cpu().numpy()\n",
    "        \n",
    "        return last_hidden_states[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3eafb5c1-7e66-45fb-8467-4f0460e14ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructorEmbeddings(Embeddings):\n",
    "   \n",
    "    def __init__(self):       \n",
    "        self.model = embedding_functions.InstructorEmbeddingFunction(model_name=\"hkunlp/instructor-base\", device=\"cuda\")\n",
    "       \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.model(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07479bb1-d597-4159-b53e-a60b8d191a65",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b81fe05e-37c0-4e67-aeae-9802845e57cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = pd.read_pickle(f'{shared_dir}/data/course_info.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b18015-4ffe-4d18-9361-92151a29edea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{shared_dir}/data/courses_info_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "749b3ab0-c7e7-4781-b17e-e2aaa654618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_bio = df[df['title_en'] == \"Bioinformatics Seminar\"]['all_course_info'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10b69336-8f27-4782-b898-77633dcd853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "loader = DataFrameLoader(df, [\"title_en\", \"all_course_info\"])\n",
    "# chunk size must be 512 because InstructorEmbeddings max_seq_length = 512\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "#embeddings = DistilbertEmbeddings()\n",
    "embeddings = InstructorEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "880c83fd-09bc-4f44-a29e-ae3f3fad0a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IGNORED] [2853] [title_en] nan\n",
      "[IGNORED] [2857] [title_en] nan\n"
     ]
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28106217-50f4-4fbd-9c42-334b6baeb884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title_en', 'all_course_info'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76ec4d92-c328-4917-b843-3f687deb5e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Private International Law\\nThe name of the course is Private International Law. The purpose of the course is  The purpose of the course is to teach the theoretical principles of private international law (PIL); to explain the key-notions of PIL and the development of its main institutes; to give an understanding of the main fields and terminology of PIL; to teach students to use the instruments of PIL to determine the international jurisdiction of courts and the applicable law to a given dispute, as well as to assess the possibilities of recognizing and enforcing foreign judgments in Estonia. At the end of the course the student is able to:The course covers the main topics of private international law (PIL), its terminology and development. The instruments which govern the determination of international jurisdiction and the applicable law in Estonia, as well as their most relevant norms, will be covered with the aim of teaching students to use them in practice (the Hague conventions, other international agreements that are binding to the Republic of Estonia, relevant EU law and Estonian legal acts).Language of instruction is Estonian. The course is offered by Department of Private Law. The course is taught in 2023/2024 years. The course is a Regular course. The course is offered for the folowing study levels: master. Number of seminar hours is 16. Number of independent work hours is 140.0. Number of credits is 6.0. The course lecturers are: Gea Lepik.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc986488-6d8b-44cc-82a0-227c72fb96f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_documents([texts[0].page_content])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78e7dc2c-f563-4a9a-9318-f9d29bc8324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# this will create the chroma embedding database!!!\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac42fd64-a44f-4ae1-b691-9e560b2a9d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Seminar on Natural Language Processing\\nThe name of the course is Seminar on Natural Language Processing. The purpose of the course is  The student will get a quick and painful hands-on acquaintance and experience with one of actual problems of NLP by participating in one of the international competitions on machine translation, dependency parsing, semantic processing, etc.2018-2019 Fall  seminar\\'s topic will be about Natural Language Processing and Wikipedia. As we all know, the advancement in NLP relies on good quality data.  In the last years, Wikipedia, the largest collaborative online encyclopedia, is used more and more for major NLP tasks. Wikipedia is not simply a corpus but it has a semantic structure that can be exploited in various ways (e.g. it each Wikipedia page is tagged with a set of categories, homonyms take us to a disambiguation page, Wikipedia contains infoboxes that can be used in supervised learning etc ...) In the seminar, we will explore some of the ways in which the researchers make use of Wikipedia. The student has acquired practical and theoretical knowledge on the covered topic in NLP, its state-of-the-art methods, their limitations and possibilities.This is a graduate level seminar that differs between Spring and Fall semesters: in the Fall semester we focus on a specific area of natural language processing and learn about it\\'s background and state-of-the-art in-depth.In Spring we participate in international competitions called \"shared tasks\" on machine translation, dependency parsing, semantic processing and and others. Depending on the interests of the students the issues discussed will include (but will not be limited to) the following:Using Wikipedia knowledge for Name Entity Disambiguation and Word Sense Disambiguation.Using Wikipedia for multi-document summarization.  Using Wikipedia for Question Answering.Using Wikipedia for automatic relation extraction.Linking Wikipedia with other resources such as Wordnet and various taxonomies.Automatic extraction of taxonomies from Wikipedia.Computing Semantic Relatedness using  Wikipedia.What is Wikification and how can it be used.As you can see from the short summary above we are targeting a wide variety of NLP tasks. The students will receive a relevant article to read and one of them will present the article at the next meeting. Then, we will discuss together the article.Language of instruction is Estonian. The course is offered by Chair of Natural Language Processing. The course is taught in 2023/2024 years. The course is a Regular course. The course is offered for the folowing study levels: master,doctoral. Number of seminar hours is 32. Number of independent work hours is 46.0. Number of credits is 12.0. The course lecturers are: Mark Fišel, Agnes Luhtaru.', metadata={}),\n",
       "  0.2345506250858307),\n",
       " (Document(page_content='Natural Language Processing\\nThe name of the course is Natural Language Processing. The purpose of the course is  The goal of this subject is to teach students the contemporary natural language processing tasks and methods; to show how to formulate various NLP related problems as appropriate text mining or machine learning tasks; to practice implementing solutions to these tasks using appropriate tools and techniques. After passing the course the studentThe course introduces basic natural language processing tasks such as language modeling, text classification, and sequence models. Tasks are formulated in deep-learning terms, using recurrent and attention-based neural networks. In addition, other topics relevant to natural language processing are discussed: text preprocessing, various text representation methods, methods for evaluating systems results etc.Language of instruction is Estonian. The course is offered by Chair of Natural Language Processing. The course is taught in 2023/2024 years. The course is a Regular course. The course is offered for the folowing study levels: master,doctoral. Number of lecture hours is 20.0. Number of practice hours is 28. Number of seminar hours is 12. Number of independent work hours is 96.0. Number of credits is 6.0. The course lecturers are: Kairit Sirts, Emil Kalbaliyev, Aleksei Dorkin.', metadata={}),\n",
       "  0.25872087478637695),\n",
       " (Document(page_content=\"Machine Learning and Neural Networks\\nThe name of the course is Machine Learning and Neural Networks. The purpose of the course is  This course provides a broad introduction to machine learning, data mining and statistical pattern recognition. Within this course, students learn the most effective machine learning methods and can practice their independent deployment and recruitment. They also learn how to apply these methods quickly and skillfully to solve new problems.The course includes:1) Supervised instruction (parametric / non-parametric algorithms, machine's support vector , cores, neural networks).2) Non-supervised learning (clustering, dimensional reduction, recommendation system, in-depth training).3) Best practice in machine learning (bias-variance theory, innovation process in machine learning and artificial intelligence).The course also relies on a number of sample-case solutions and applications, so that the student learns how to implement machine learning algorithms for building smart robots (perception, control), understanding the text (web search, spam), computer vision, data mining and other areas.Language of instruction is Estonian. The course is offered by Narva College. The course is taught in 2023/2024 years. The course is a Regular course. The course is offered for the folowing study levels: applied. Number of practice hours is 32. Number of independent work hours is 46.0. Number of credits is 3.0. The course lecturers are: Andre Sääsk.\", metadata={}),\n",
       "  0.2612687051296234),\n",
       " (Document(page_content='Introduction to Neuromorphic Computing\\nThe name of the course is Introduction to Neuromorphic Computing. The purpose of the course is  The aim of the course is to cover the basics of Neuromorphic computing which is inspired by the structure and behavior of the human brain. It utilizes nanoscale electronic circuits to mimic the way neurons process information. It is a subfield of artificial intelligence and has the potential to provide much more powerful and efficient computing than traditional digital computing. Neuromorphic computing uses digital circuits to mimic the neurons and synapses of the brain, which makes it a powerful tool for various applications, such as machine learning, robotics, and image processing. Neuromorphic computing has the potential to enable faster, more accurate and more efficient computing than conventional computing technologies. This type of computing has the potential to revolutionize the way we interact with machines. It could be used to develop autonomous systems that can learn from their environment and make decisions based on their experiences. After finishing the course studentsThis course provides an accessible pathway for all levels of learners looking to break into the neuromorphic computing space or apply it to their own projects. This course gives a brief overview of the basic concepts, principles, and practice of neuromorphic computing. The main goal is to learn to plan and carry out practical neuromorphic related projects. The main stages of neuromorphic computing are discussed, and available software tools reviewed.The course is in English. We are dealing with concepts that achieve top performance results.Lastly, one of our top priorities is to have visually appealing examples throughout the course. Basic knowledge in Python programming is expected but not mandatory.Language of instruction is Estonian. The course is offered by Department of Experimental Physics. The course is taught in 2022/2023 years. The course is a Regular course. The course is offered for the folowing study levels: bachelor,master,doctoral. Number of lecture hours is 16.0. Number of seminar hours is 16. Number of independent work hours is 46.0. Number of credits is 3.0. The course lecturers are: Tiia Lillemaa, Amudhavel Jayavel.', metadata={}),\n",
       "  0.2667979896068573)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score(\"NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3e0933-3f99-4042-9019-5f4eb7eca3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/gpfs/space/home/zaliznyi/miniconda3/envs/nlp/bin/python3.10 -m pip install bitsandbytes\n",
    "#!/gpfs/space/home/zaliznyi/miniconda3/envs/nlp/bin/python3.10 -m pip uninstall tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25129f53-87d5-486f-a244-4c54553052fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.45s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaWrapperModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3599eb7-7d4d-4902-8412-dcc8a1f1e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Your task is to provide students with information about courses offered in University of Tartu. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b229b64f-4736-4eab-a559-06de223ad241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANNOT LOAD FROM DISK BECAUSE OF EMBD DIM INITIALIZATION BUG\n",
    "# db = Chroma(persist_directory=f\"{shared_dir}/data/chroma\", embedding_function=embeddings)  # load from disk \n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs = {'k': 3}\n",
    "chain_type_kwargs = {\"prompt\": llm_prompt}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs)\n",
    "qa.combine_documents_chain.document_prompt = chain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8970ea21-5cc4-4852-85a5-d38b2a808d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Time to tokenize:  00:00:00\n",
      "Generating...\n",
      "Time to generate:  00:13:09\n",
      "Decoding...\n",
      "Time to decode:  00:00:00\n",
      "{'query': 'What is the purpose of Bioinformatics Seminar?', 'result': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nBioinformatics Seminar\\nThe name of the course is Bioinformatics Seminar. The purpose of the course is  To get acquainted with modern bioinformatics research publications and presentation of the results therein Upon successful completion of this course, students should be able to:The seminar is organized in the form of a Journal Club. Scientific results are presented and discussed. The topics covered will vary from semester to semester. Please see the seminar website on http://courses.cs.ut.ee for more details.Language of instruction is Estonian. The course is offered by Chair of Data Science. The course is taught in 2023/2024 years. The course is a Regular course. The course is offered for the folowing study levels: master,doctoral. Number of seminar hours is 24. Number of independent work hours is 54.0. Number of credits is 12.0. The course lecturers are: Kaur Alasoo..\\n\\nBioinformatics for Bioengineers\\nThe name of the course is Bioinformatics for Bioengineers. The purpose of the course is  The main aims of the course are to i) provide a basic general overview of the bioinformatics methods and relevant databases used to analyze digital genetic information and gene regulatory mechanisms; ii) to introduce the general concept and methodology for designing synthetic biological systems via genome engineering; iii) design a synthetic chromosome in silico that will serve as a blueprint for construction of the synthetic chromosome in a connected \"build-a-genome-course\" within the \"LTTI.00.007 Synthetic Biology Laboratory Course\". By the end of the course participants will know relevant databases for biological data (incl. genomic regions, protein domains, protein-protein interactions, gene expression data, Gene Ontology). The course will give an overview of the main bioinformatics methods that the attendee can later use in his/her research projects that involve the analysis of genetic information and/or design of novel genetic circuits. The course offers an alternative to the classical form of bioinformatics courses in terms of additional modules focused on genome engineering and rational design of chromosomes. It also introduces an innovative Synthetic Biology Open Language (SBOL):  a community standard for communicating designs in synthetic biology.The course consists of three modules: I - Introductory bioinformatics module*\\tIn silico analysis of single genes and proteins - basic search algorithms (blast), retrieval options, file types, alignment options, phylogenetic tree building etc. *\\tDesign of primers for single genes - PCR primers, cDNA cloning primers *\\tAnalysis of genomic regions - enhancers, promoters, coding regions, intron-exon. structure, (alternative) splicing, overlapping genes (including miRNA genes) *\\tAnalysis of gene expression data (RNA seq, microarray, protein arrays, chip data). *\\tAnalysis of interactome and GO data.*\\tAnalysis of protein interaction networks.II - Genome engineering module*\\tThe Synthetic Biology Open Language (SBOL):  a community standard for communicating designs in synthetic biology. *\\tSequence manipulations, genome editing, codon optimization *\\tIn silico implementation of synthetic gene networks and functional modules based on genetic information from various species*\\tDesign of synthetic protein interaction networks using the Short Linear Motif (SLiMs) databases*\\tDesign of logic gates - outreach to computer engineering*\\tIn silico protein engineering *\\tAssembly of heterologous biosynthetic pathways in cell factory producer strains and cell lines; integrating across the Design-Build-Test-Cycle.*\\tBioinformatics for the synthetic biology of natural products. *\\tMetabolic modeling of engineered microbial cell factory strains. III - Build-a-genome-course pre-planning module:*\\tThe genome SCRAMBLe method*\\tIntroduction to the Phosphoprocessors toolbox*\\tDesign of a chromosome structure and the minichunk recombination strategy*\\tSetting up a practical timeline and performing a cost analysis of the chromosome synthesis.Language of instruction is Estonian. The course is offered by Institute of Technology. The course is taught in 2023/2024 years. The course is a Regular course. The course is offered for the folowing study levels: bachelor. Number of lecture hours is 52.0. Number of practice hours is 20. Number of independent work hours is 84.0. Number of credits is 6.0. The course lecturers are: Mart Loog, Arto Pulk, Ervin Valk, Oleg Košik, Rait Kivi, Kaur Pääbo..\\n\\nBioinformatics\\nThe name of the course is Bioinformatics. The purpose of the course is  This course aims at introducing students to the concepts and principles of bioinformatics, starting from the biological problem statements to the algorithmic methods Upon successful completion of this course, students should be able to:- understand the main broad  research areas of bioinformatics- start performing research in bioinformaticsThis course provides an introduction to bioinformatics (for mostly the computer science and statistics students). Topics include the DNA, RNA, and proteins, sequence and structure vased methods, genetic networks, and systems biology basics. Additionally, main new high throughput gene technology techniques and data sets are discussed.Language of instruction is Estonian. The course is offered by Chair of Data Science. The course is taught in 2023/2024 years. The course is a Regular course. The course is offered for the folowing study levels: master,doctoral. Number of lecture hours is 32.0. Number of practice hours is 32. Number of independent work hours is 92.0. Number of credits is 6.0. The course lecturers are: Priit Adler, Erik Abner, Kaur Alasoo, Ralf Tambets..\\n\\nBioinformatics and Genomics\\nThe name of the course is Bioinformatics and Genomics. The purpose of the course is  The course gives review about methods used in bioinformatics and familiarize students with new scientific adventures opened by availability of full genome DNA sequences After taking a given course, students: This course is meant to those whose everyday work is not related to computational biology. During this course students will learn to use several web based and Linux command-line tools and algorithms behind these tools.The following topics are covered:- bioinformatics databases- sequencing, annotation, comparison, and evolution of genomes- sequence alignment- PCR primer design- Linux command-line tools- Perl scripting- sequence analysis: genome assembly, RNA-Seq, MetagenomicsLanguage of instruction is Estonian. The course is offered by Chair of Bioinformatics. The course is taught in 2022/2023 years. The course is a Regular course. The course is offered for the folowing study levels: bachelor. Number of lecture hours is 6.0. Number of practice hours is 14. Number of independent work hours is 136.0. Number of credits is 6.0. The course lecturers are: Reidar Andreson, Maido Remm..\\n\\nQuestion: What is the purpose of Bioinformatics Seminar?\\nHelpful Answer:\\nThe seminarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinarinar.\\nthe answer the answer the answer the answer the answer the answer.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nof the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nknow\\nknow\\n\\n\\nanswer\\n\\n\\n\\n\\nknow and a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nof the\\nof the question of the answer.\\nthe question.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nthe question, the answer, the answer, and the first.\\n\\n\\nthe, and the question.\\n\\n\\nthe question.\\nand the\\nand the question and the\\nand the\\nand the\\n.\\n\\n\\n\\nthe answer.\\nthe answer, and the answer, and the\\nand the \\nthe, and the 10.\\nof the 10.\\nthe 10.\\nto answer to answer to answer to the following to the 1stiffie is the answer the answer the answer, and the answer and the and the and the answer and the answer.\\nanswer.\\nanswer, the answer, the answer, the answer, the\\nthe, the\\n.\\n.\\nthe\\nthe\\nthe\\nthe\\nthe\\nthe\\nwith the\\nthe\\n.\\n.\\nto the\\n\\nof the\\nof the\\nof the\\nthe\\n\\nthe\\nthe\\nand, the and the and the and the\\nand and the\\nand of the answer.\\nandially.\\nand and the answer and the\\nto the\\nto\\nand and the\\nto the\\nto the\\nto the\\nto of the\\nthe to the\\nand is is is is is the\\nand is, and the\\nand,\\nand, and the, and the, and the 1.\\nof the 1 of the 1 of the of the 1.\\nand of the\\nand of the 1.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n[the, the, the, the 2. 1.\\n.\\nof the 1. the, the, the, the, the, the, the, the 1. the 1. 1. 1.\\n.\\n.\\n.\\n.\\n.\\n.\\n. 1. 1. 1. 1.\\n.\\n.\\n.\\n.\\nand.\\nand.\\nand. of the 1 of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the to the to the  to the .  of  of  of  of  of  of  of the . 1. 1. 1. 2. 2. 2. 2 of the of the of the [answer at the, 1, and, and. and.\\nto. 1. 1. of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the are of the [ are [ are of the are of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of of of of the of the of the of the of the of the [. [. [. . . . . of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the and the and the and the and the . . . the. the the the the . the . of the of the of the of the of of the of of of of of of the of of the of the of the, [[. [. [.,,, [. [. the, the, the, the, the, the. the and the and of the of the of of the of the of the of the of the of the of the of the of the of the of the at the at the  of the  of the of the of the of the . 1 of the-[ of the [. [[ of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the for the for the for the for the for the for the of the to- to for the to for the- [- [- of the of the of the of the of the of the of of the of the of the, the, the, the [- are [. [- [- [ of [- [ of the of the-c-c of the-c of the of the of the of the of of of the of the of the of the-c of the-c-1-d-t.. of. of the of of of of the of the of the of of the of of of of of the of the of of of of of of of of of of of of of of of of of of the of- of- of- of- of- of the of of the of the- the-the-the-the- of- of the of of of of of of of of of of of of of of of of of of of of of of of of- and- and- and- and- and- of- of- of- of- of- of- of- of- of. of. of- of- of- of- of of of of of of of of of of of of- of- of- of- of- of- of the of of the of the of the-1 of the of the of the of the. of the of. of. of. of. of. of. of the of the of of of. of the of the of the of the of of the of the of the of the of the of the-[-[-[ of the of the of the of the of of the of the of the of the of the of the of the of the of the of the of the of the of the of the of of of of of of the of the of the of-b of-[. of- of- of- for- for-for-of- of of of of of of of the of the of the of the of for of for of the of ( of- of- of- of- of of of the of the of the of of of of of of of of of of of of of of the of the of the of the of the of the of the of the of the of the of the of the of- of- of- of- of-of-d-d-d of-of of-of of-t of-t of- of-to-to-to-to-t-t-d-d- of the of the-d-d- of- of- of-[ of-[ of-1 of [ of for of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of-of-of-0-0-0-0-1 of-0 of the of the of-of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of the of the of of of of of of of of of of. of. of- of-of-of-of-of of of of of of of of of of of of of of of of of of of of of of of-of of-c of-[-[[[[[[[[[of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of-of of-c of-c of-to-c-c-h-h-of-of-of-of-d-d-1 of-c of the of the of the of-c of-c of-c of-to-to of-of-of-of-c of-of of of of of of of of-of-of-of-of of of of of of of of of of of of of of of of of of of of of of-c of of of of of of of of of of of of of of-1 of-1 of-1 of-1 of-c-c-c-c-c-c of [[of of of of of of of of of of of of of-c of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of [1 of of of of of of ( of (1 of (1 of-c of of of of of of-c of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of [B-B-B of-B of of of of of of of of of of of of of of of to to to to to of C of C-C-C of of of of of of of of of of of of of of of of-B of-B of-B of of ( of of of of of ( of ( of ( of of of C of C-B of C-C of C-C of C of C-C of of C of C of of C of C of C of [C of\\tB of [B-C-1-1-1-\\t\\t\\t\\t\\t\\t\\t\\t\\t1 [1 [1 [1 of [1 of\\t1 of C-C-C-C-1-1-C-C-1-1 of-1 of-C-C-C-C-B-B-C-C-C P-C-C-C [C [C: C: C:\\n1:\\n(\\n(1:\\n: C: C:\\n:\\n: [C:\\n:\\n:\\n:\\n:\\n:\\t:\\tA:\\tB:\\tA:\\n\\tA:\\n:\\n\\tP of [P of [1 of [1 of [1 of [1 of [1 [1 [1 of [1 of [1 [11\\t1\\t1\\t1\\t1\\t1\\t1\\t\\t1 ['}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the purpose of Bioinformatics Seminar?\"\n",
    "print(qa(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4072eb3-045b-44d3-8047-497f8fbfb2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}