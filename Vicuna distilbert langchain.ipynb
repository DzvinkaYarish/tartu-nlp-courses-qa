{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a933ecd-5d4c-461c-bcd3-f46e08f6400d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  experiments  llama  llama_langchain  tartu-nlp-courses-qa\n"
     ]
    }
   ],
   "source": [
    "!ls /gpfs/space/projects/stud_ml_22/NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035e9ace-a01e-4d35-96be-019cbbd19e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_exploration.py  main.py  requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /gpfs/space/projects/stud_ml_22/NLP/llama_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96371d2e-ea5e-420a-85e8-453a228b9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/en/latest/getting_started/getting_started.html\n",
    "# https://python.langchain.com/en/latest/use_cases/question_answering.html\n",
    "# https://python.langchain.com/en/latest/modules/indexes/getting_started.html\n",
    "# https://python.langchain.com/en/latest/use_cases/question_answering/semantic-search-over-chat.html\n",
    "\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.schema import Generation\n",
    "from langchain.schema import PromptValue, LLMResult\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# langchain.document_loaders.DataFrameLoader has a quite a limited functionality\n",
    "class DataFrameLoader(BaseLoader):\n",
    "    def __init__(self, data_frame: Any, page_content_columns: List[str]):\n",
    "        if not isinstance(data_frame, pd.DataFrame):\n",
    "            raise ValueError(\n",
    "                f\"Expected data_frame to be a pd.DataFrame, got {type(data_frame)}\"\n",
    "            )\n",
    "        self.data_frame = data_frame\n",
    "        self.page_content_columns = page_content_columns\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        result = []\n",
    "        for i, row in self.data_frame.iterrows():\n",
    "            text = \"\"\n",
    "            metadata = {}\n",
    "            for col in self.page_content_columns:\n",
    "                data = row[col]\n",
    "                if isinstance(data, list):\n",
    "                    text += \"\".join(data) + \"\\n\"\n",
    "                elif isinstance(data, str):\n",
    "                    text += data + \"\\n\"\n",
    "                else:\n",
    "                    print(f\"[IGNORED] [{i}] [{col}] {data}\")\n",
    "\n",
    "            metadata_temp = row.to_dict()\n",
    "            for col in self.page_content_columns:\n",
    "                metadata_temp.pop(col)\n",
    "            # Metadata is a dict where a value can only be str, int, or float. Delete other types.\n",
    "            for key, value in metadata_temp.items():\n",
    "                if isinstance(value, (str, int, float)):\n",
    "                    metadata[key] = value\n",
    "\n",
    "            result.append(Document(page_content=text, metadata=metadata))\n",
    "        return result\n",
    "\n",
    "\n",
    "class MyLanguageModel(BaseLanguageModel):\n",
    "    def generate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                        callbacks: Callbacks = None) -> LLMResult:\n",
    "        generation = Generation(text=\"Hello World!\")\n",
    "        result = LLMResult(generations=[[generation]])\n",
    "        return result\n",
    "\n",
    "    async def agenerate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                               callbacks: Callbacks = None) -> LLMResult:\n",
    "        pass  # \"whatever dude\"\n",
    "\n",
    "\n",
    "# NOTE: the OpenAIEmbeddings embeddings have the dimensionality of 1536\n",
    "class MyEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [[1.0] * 1536, [2.0] * 1536]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return [1.0] * 1536\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5cecf3a8-2fff-4dc3-910f-6db44ccea08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_dir = '/gpfs/space/projects/stud_ml_22/NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b81fe05e-37c0-4e67-aeae-9802845e57cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>code</th>\n",
       "      <th>parent_uuid</th>\n",
       "      <th>parent_code</th>\n",
       "      <th>parent_credits</th>\n",
       "      <th>title_en</th>\n",
       "      <th>general_input_languages</th>\n",
       "      <th>general_structural_unit_shares</th>\n",
       "      <th>general_year.en</th>\n",
       "      <th>general_type.code</th>\n",
       "      <th>...</th>\n",
       "      <th>resources_mandatory_materials</th>\n",
       "      <th>resources_recommended_materials</th>\n",
       "      <th>resources_learning_environments</th>\n",
       "      <th>participants_lecturers</th>\n",
       "      <th>participants_assistants</th>\n",
       "      <th>schedule_entries</th>\n",
       "      <th>schedule_weeks.et</th>\n",
       "      <th>registration_info_min_students</th>\n",
       "      <th>registration_info_max_students</th>\n",
       "      <th>registration_info_audience.en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a198ed66-1fb5-4f7e-ee43-7fbbf5c09aca</td>\n",
       "      <td>sv-2023-spring-openuniv</td>\n",
       "      <td>b99c0bb1-efd4-9b0a-857a-3dc7114e5c19</td>\n",
       "      <td>OIEO.06.046</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Private International Law</td>\n",
       "      <td>[{'language_code': 'et', 'language_name': 'Est...</td>\n",
       "      <td>[{'code': 'SVOI04', 'name': 'Department of Pri...</td>\n",
       "      <td>2023/2024</td>\n",
       "      <td>regular</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'person_uuid': 'd7a3f19b-d7c7-fbe5-b41b-e5e3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'course_week': 1, 'work_type': {'code': 'lec...</td>\n",
       "      <td>Nädalad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6ee943ab-a839-a937-76c0-2e0e4daedb8b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76162416-d608-f48f-ec5d-5c40ce9b320d</td>\n",
       "      <td>FLFI.00.016</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Doctoral Seminar</td>\n",
       "      <td>[{'language_code': 'et', 'language_name': 'Est...</td>\n",
       "      <td>[{'code': 'HVFI01', 'name': 'Department of Phi...</td>\n",
       "      <td>2023/2024</td>\n",
       "      <td>regular</td>\n",
       "      <td>...</td>\n",
       "      <td>Presentations.</td>\n",
       "      <td>Ask the supervisor.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'person_uuid': '4b6a00ae-35fd-a00e-d38e-0e2f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'work_type': {'code': 'colloquium', 'et': 'k...</td>\n",
       "      <td>24-40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7fc4f6cf-f011-91e8-4c42-abbd782a4a2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31c327d5-2b61-c764-b418-bda22c577265</td>\n",
       "      <td>SVNC.00.179</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pedagogical Practicum</td>\n",
       "      <td>[{'language_code': 'et', 'language_name': 'Est...</td>\n",
       "      <td>[{'code': 'SVNC', 'name': 'Narva College', 'co...</td>\n",
       "      <td>2023/2024</td>\n",
       "      <td>practice</td>\n",
       "      <td>...</td>\n",
       "      <td>Põhikooli- ja gümnaasiumi Riiklik õppekava htt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'person_uuid': 'e664d700-4a63-b159-794e-d0be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'work_type': {'code': 'practice', 'et': 'pra...</td>\n",
       "      <td>24-43</td>\n",
       "      <td>15.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d72f2ef7-d264-eceb-b759-a9a66cc27593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0e7d0b5d-83ea-f260-7e09-c3d59ea9c250</td>\n",
       "      <td>KKSB.05.092</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Practice in the Work Environment</td>\n",
       "      <td>[{'language_code': 'et', 'language_name': 'Est...</td>\n",
       "      <td>[{'code': 'MVSF', 'name': 'Institute of Sport ...</td>\n",
       "      <td>2022/2023</td>\n",
       "      <td>regular</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'person_uuid': '1ff846ac-79c1-ef64-3910-5131...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'time': '2023-06-20', 'work_type': {'code': ...</td>\n",
       "      <td>40-52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2a69334b-ebec-b332-d5f2-984869620c04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f465e112-e552-a3d1-5fa6-e26e661b288b</td>\n",
       "      <td>MTAT.03.242</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Bioinformatics Seminar</td>\n",
       "      <td>[{'language_code': 'et', 'language_name': 'Est...</td>\n",
       "      <td>[{'code': 'LTAT02', 'name': 'Chair of Data Sci...</td>\n",
       "      <td>2023/2024</td>\n",
       "      <td>regular</td>\n",
       "      <td>...</td>\n",
       "      <td>The readings for every topic could be found on...</td>\n",
       "      <td>The readings for every topic could be found on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'person_uuid': '643ca845-067b-f270-23fd-dafe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'work_type': {'code': 'seminar', 'et': 'semi...</td>\n",
       "      <td>24.-39. õppenädalal</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid                     code  \\\n",
       "0  a198ed66-1fb5-4f7e-ee43-7fbbf5c09aca  sv-2023-spring-openuniv   \n",
       "1  6ee943ab-a839-a937-76c0-2e0e4daedb8b                      NaN   \n",
       "2  7fc4f6cf-f011-91e8-4c42-abbd782a4a2a                      NaN   \n",
       "3  d72f2ef7-d264-eceb-b759-a9a66cc27593                      NaN   \n",
       "4  2a69334b-ebec-b332-d5f2-984869620c04                      NaN   \n",
       "\n",
       "                            parent_uuid  parent_code  parent_credits  \\\n",
       "0  b99c0bb1-efd4-9b0a-857a-3dc7114e5c19  OIEO.06.046             6.0   \n",
       "1  76162416-d608-f48f-ec5d-5c40ce9b320d  FLFI.00.016            15.0   \n",
       "2  31c327d5-2b61-c764-b418-bda22c577265  SVNC.00.179             4.0   \n",
       "3  0e7d0b5d-83ea-f260-7e09-c3d59ea9c250  KKSB.05.092             3.0   \n",
       "4  f465e112-e552-a3d1-5fa6-e26e661b288b  MTAT.03.242            12.0   \n",
       "\n",
       "                           title_en  \\\n",
       "0         Private International Law   \n",
       "1                  Doctoral Seminar   \n",
       "2             Pedagogical Practicum   \n",
       "3  Practice in the Work Environment   \n",
       "4            Bioinformatics Seminar   \n",
       "\n",
       "                             general_input_languages  \\\n",
       "0  [{'language_code': 'et', 'language_name': 'Est...   \n",
       "1  [{'language_code': 'et', 'language_name': 'Est...   \n",
       "2  [{'language_code': 'et', 'language_name': 'Est...   \n",
       "3  [{'language_code': 'et', 'language_name': 'Est...   \n",
       "4  [{'language_code': 'et', 'language_name': 'Est...   \n",
       "\n",
       "                      general_structural_unit_shares general_year.en  \\\n",
       "0  [{'code': 'SVOI04', 'name': 'Department of Pri...       2023/2024   \n",
       "1  [{'code': 'HVFI01', 'name': 'Department of Phi...       2023/2024   \n",
       "2  [{'code': 'SVNC', 'name': 'Narva College', 'co...       2023/2024   \n",
       "3  [{'code': 'MVSF', 'name': 'Institute of Sport ...       2022/2023   \n",
       "4  [{'code': 'LTAT02', 'name': 'Chair of Data Sci...       2023/2024   \n",
       "\n",
       "  general_type.code  ...                      resources_mandatory_materials  \\\n",
       "0           regular  ...                                                NaN   \n",
       "1           regular  ...                                     Presentations.   \n",
       "2          practice  ...  Põhikooli- ja gümnaasiumi Riiklik õppekava htt...   \n",
       "3           regular  ...                                                NaN   \n",
       "4           regular  ...  The readings for every topic could be found on...   \n",
       "\n",
       "                     resources_recommended_materials  \\\n",
       "0                                                NaN   \n",
       "1                                Ask the supervisor.   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  The readings for every topic could be found on...   \n",
       "\n",
       "   resources_learning_environments  \\\n",
       "0                              NaN   \n",
       "1                              NaN   \n",
       "2                              NaN   \n",
       "3                              NaN   \n",
       "4                              NaN   \n",
       "\n",
       "                              participants_lecturers participants_assistants  \\\n",
       "0  [{'person_uuid': 'd7a3f19b-d7c7-fbe5-b41b-e5e3...                     NaN   \n",
       "1  [{'person_uuid': '4b6a00ae-35fd-a00e-d38e-0e2f...                     NaN   \n",
       "2  [{'person_uuid': 'e664d700-4a63-b159-794e-d0be...                     NaN   \n",
       "3  [{'person_uuid': '1ff846ac-79c1-ef64-3910-5131...                     NaN   \n",
       "4  [{'person_uuid': '643ca845-067b-f270-23fd-dafe...                     NaN   \n",
       "\n",
       "                                    schedule_entries    schedule_weeks.et  \\\n",
       "0  [{'course_week': 1, 'work_type': {'code': 'lec...              Nädalad   \n",
       "1  [{'work_type': {'code': 'colloquium', 'et': 'k...                24-40   \n",
       "2  [{'work_type': {'code': 'practice', 'et': 'pra...                24-43   \n",
       "3  [{'time': '2023-06-20', 'work_type': {'code': ...                40-52   \n",
       "4  [{'work_type': {'code': 'seminar', 'et': 'semi...  24.-39. õppenädalal   \n",
       "\n",
       "   registration_info_min_students  registration_info_max_students  \\\n",
       "0                             1.0                            60.0   \n",
       "1                             1.0                             NaN   \n",
       "2                            15.0                            46.0   \n",
       "3                             1.0                            80.0   \n",
       "4                             5.0                             NaN   \n",
       "\n",
       "   registration_info_audience.en  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(f'{shared_dir}/data/course_info.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "775d3f22-b482-476a-bf6a-fd9a43292e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(encoder_only = False):\n",
    "        PATH_TO_CONVERTED_WEIGHTS = os.path.join(\n",
    "            shared_dir, \"llama/7B_Vicuna_added/\")\n",
    "\n",
    "        device = torch.device(\n",
    "            \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        config = AutoConfig.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
    "        config.max_position_embeddings = 1024\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            PATH_TO_CONVERTED_WEIGHTS,\n",
    "            config=config,\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=not args.no_gradient_checkpointing,\n",
    "            load_in_8bit=True,\n",
    "            device_map={\"\": Accelerator().process_index},\n",
    "            # device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abb63f2b-d204-4367-9968-3782a4990059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da61bb46-b496-4482-8439-682aa8302ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from accelerate import Accelerator\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "class LlamaWrapperModel(BaseLanguageModel):\n",
    "\n",
    "    def generate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                        callbacks: Callbacks = None) -> LLMResult:\n",
    "\n",
    "        model = get_model()\n",
    "        PATH_TO_CONVERTED_TOKENIZER = os.path.join(\n",
    "            shared_dir, \"llama/7B_converted/\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
    "              \n",
    "        with torch.no_grad():\n",
    "            prompt = prompts[0].text\n",
    "            \n",
    "            print(\"Tokenizing...\")\n",
    "            s = time.time()\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            e1 = time.time()\n",
    "            print(\"Time to tokenize: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e1 - s)))\n",
    "            \n",
    "            print(\"Generating...\")\n",
    "            generate_ids = model.generate(input_ids=inputs.input_ids.to(\n",
    "                device), max_length=5000)  # max_length = max_new_tokens + prompt_length\n",
    "            e2 = time.time()\n",
    "            print(\"Time to generate: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e2 - e1)))\n",
    "            \n",
    "            print(\"Decoding...\")\n",
    "            text_result = tokenizer.batch_decode(\n",
    "                generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            e3 = time.time()\n",
    "            print(\"Time to decode: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e3 - e2)))\n",
    "\n",
    "        generation = Generation(text=text_result)\n",
    "        result = LLMResult(generations=[[generation]])\n",
    "        return result\n",
    "\n",
    "    async def agenerate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                               callbacks: Callbacks = None) -> LLMResult:\n",
    "        pass  # \"whatever dude\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe38752-36ad-4250-a687-620573c315dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the OpenAIEmbeddings embeddings have the dimensionality of 1536\n",
    "class DistilbertEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        # Tokenize the messages and generate embeddings\n",
    "        tokenized = [tokenizer.encode(text, add_special_tokens=True, max_length=1024, truncation=True) for text in texts]\n",
    "        padded = np.array([i + [0]*(512-len(i)) for i in tokenized.values])\n",
    "        input_ids = torch.tensor(padded)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids)[0][:,0,:].numpy()\n",
    "        \n",
    "        return last_hidden_states\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        # Tokenize the messages and generate embeddings\n",
    "        tokenized = tokenizer.encode(text, add_special_tokens=True, max_length=1024, truncation=True)\n",
    "        padded = np.array([i + [0]*(512-len(i)) for i in tokenized.values])\n",
    "        input_ids = torch.tensor(padded)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids)[0][:,0,:].numpy()\n",
    "        \n",
    "        return last_hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d739df5-3f44-46dd-bac2-36069e3c3f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "chroma-collections.parquet  chroma-embeddings.parquet  index\n"
     ]
    }
   ],
   "source": [
    "!ls /gpfs/space/projects/stud_ml_22/NLP/data/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59f1fea4-1f66-4a7a-b1d0-d8613892d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/space/projects/stud_ml_22/NLP/data/chroma\n"
     ]
    }
   ],
   "source": [
    "print(f\"{shared_dir}/data/chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10b69336-8f27-4782-b898-77633dcd853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df, [\"title_en\", \"overview_objectives\",\n",
    "                              \"overview_learning_outcomes\", \"overview_description.en\"])\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = DistilbertEmbeddings()\n",
    "\n",
    "# this will create the chroma embedding database!!!\n",
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=f\"{shared_dir}/data/chroma_distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b229b64f-4736-4eab-a559-06de223ad241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 # llm = OpenAI(temperature=0.9)</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 # llm = MyLanguageModel()</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 llm = LlamaWrapperModel()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>db = Chroma(persist_directory=<span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>shared_dir<span style=\"color: #808000; text-decoration-color: #808000\">}/data/chroma\"</span>, embedding_function=embeddings     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span>retriever = db.as_retriever()                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>lllll: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span> = Field(..., description=<span style=\"color: #808000; text-decoration-color: #808000\">\"Description for lllll field\"</span>)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 18 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lllll = <span style=\"color: #808000; text-decoration-color: #808000\">'abc'</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>PATH_TO_CONVERTED_WEIGHTS = os.path.join(                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>shared_dir, <span style=\"color: #808000; text-decoration-color: #808000\">\"llama/7B_Vicuna_added/\"</span>)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>PATH_TO_CONVERTED_TOKENIZER = os.path.join(                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pydantic.main.BaseModel.__setattr__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">405</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'LlamaWrapperModel'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'__fields_set__'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[2m# llm = OpenAI(temperature=0.9)\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[2m# llm = MyLanguageModel()\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 llm = LlamaWrapperModel()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0mdb = Chroma(persist_directory=\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mshared_dir\u001b[33m}\u001b[0m\u001b[33m/data/chroma\u001b[0m\u001b[33m\"\u001b[0m, embedding_function=embeddings     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0mretriever = db.as_retriever()                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__init__\u001b[0m:\u001b[94m18\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m│   \u001b[0mlllll: \u001b[96mstr\u001b[0m = Field(..., description=\u001b[33m\"\u001b[0m\u001b[33mDescription for lllll field\u001b[0m\u001b[33m\"\u001b[0m)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__init__\u001b[0m(\u001b[96mself\u001b[0m):                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 18 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.lllll = \u001b[33m'\u001b[0m\u001b[33mabc\u001b[0m\u001b[33m'\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 19 \u001b[0m\u001b[2m│   │   \u001b[0mPATH_TO_CONVERTED_WEIGHTS = os.path.join(                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 20 \u001b[0m\u001b[2m│   │   │   \u001b[0mshared_dir, \u001b[33m\"\u001b[0m\u001b[33mllama/7B_Vicuna_added/\u001b[0m\u001b[33m\"\u001b[0m)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 21 \u001b[0m\u001b[2m│   │   \u001b[0mPATH_TO_CONVERTED_TOKENIZER = os.path.join(                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mpydantic.main.BaseModel.__setattr__\u001b[0m:\u001b[94m405\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'LlamaWrapperModel'\u001b[0m object has no attribute \u001b[32m'__fields_set__'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# llm = OpenAI(temperature=0.9)\n",
    "# llm = MyLanguageModel()\n",
    "llm = LlamaWrapperModel()\n",
    "\n",
    "db = Chroma(persist_directory=f\"{shared_dir}/data/chroma\", embedding_function=embeddings)  # load from disk\n",
    "retriever = db.as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea162ed-69d0-4a12-961b-1cd982da8291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized model: LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "initialized tokenizer LlamaTokenizerFast(name_or_path='/gpfs/space/projects/stud_ml_22/NLP/llama/7B_converted/', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)\n",
      "Tokenizing...\n",
      "Time to tokenize:  00:00:00\n",
      "Generating...\n",
      "Time to generate:  00:00:01\n",
      "Decoding...\n",
      "Time to decode:  00:00:00\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Marketing Communications\n",
      "The purpose of this course is to give knowledge and practical skills of marketing communication.\n",
      "Knows the basics of marketing communication theory and \n",
      "practical applications.\n",
      "Knows the main principles of marketing communication\n",
      "and knows how to apply them in marketing.\n",
      "The main topics covered will be intergated marketing communications, and planning an intergated marketing communications strategy, role of personal relations management, advertising and their overall fit to marketing communications mix.\n",
      "\n",
      "Marketing Communications\n",
      "The purpose of this course is to give knowledge and practical skills of marketing communication.\n",
      "Knows the basics of marketing communication theory and \n",
      "practical applications.\n",
      "Knows the main principles of marketing communication\n",
      "and knows how to apply them in marketing.\n",
      "The main topics covered will be intergated marketing communications, and planning an intergated marketing communications strategy, role of personal relations management, advertising and their overall fit to marketing communications mix.\n",
      "\n",
      "Semiotics of Marketing\n",
      "The subject aims to introduce what marketing semiotics is. Based on the objective, the student becomes more aware of one marketing strategy - marketing semiotics. A person who is more aware of marketing semiotics notices sign systems in marketing and can analyse how meaning creation affects marketing activities.\n",
      "Subject participant:a)\torients himself in the relevant vocabulary of marketing and brand semiotics;b)\tnotices the possibilities of applying marketing and brand semiotics in life;c)\tanalyses his creation from the point of view of marketing and brand semiotics using field concepts.\n",
      "The subject is mainly based on podcasts, where you can apply new knowledge yourself in addition to the theoretical overview.\n",
      "Topics:\n",
      "Part 1 - Semiotics. Introduction. (Content: sign, model, story, culture);\n",
      "Part 2 - Marketing semiotics. (Content: marketing, advertising, trade);\n",
      "Part 3 - Brand semiotics. (Content: brand, brand identity, brand image);\n",
      "Part 4 - Let's do it ourselves - brand analysis, creative work and measurement (Content: analysis, creation and measurement of the brand story).\n",
      "\n",
      "Semiotics of Marketing\n",
      "The subject aims to introduce what marketing semiotics is. Based on the objective, the student becomes more aware of one marketing strategy - marketing semiotics. A person who is more aware of marketing semiotics notices sign systems in marketing and can analyse how meaning creation affects marketing activities.\n",
      "Subject participant:a)\torients himself in the relevant vocabulary of marketing and brand semiotics;b)\tnotices the possibilities of applying marketing and brand semiotics in life;c)\tanalyses his creation from the point of view of marketing and brand semiotics using field concepts.\n",
      "The subject is mainly based on podcasts, where you can apply new knowledge yourself in addition to the theoretical overview.\n",
      "Topics:\n",
      "Part 1 - Semiotics. Introduction. (Content: sign, model, story, culture);\n",
      "Part 2 - Marketing semiotics. (Content: marketing, advertising, trade);\n",
      "Part 3 - Brand semiotics. (Content: brand, brand identity, brand image);\n",
      "Part 4 - Let's do it ourselves - brand analysis, creative work and measurement (Content: analysis, creation and measurement of the brand story).\n",
      "\n",
      "Question: What's Jaak Vilo's last name?\n",
      "Helpful Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's Jaak Vilo's last name?\"\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de60fdd1-940c-456a-829b-b36500eb8280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_proj",
   "language": "python",
   "name": "nlp_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
