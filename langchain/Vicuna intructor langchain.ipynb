{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe30f5e-6704-4a0c-888e-fbe23ed52d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 22 17:40:23 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.106.00   Driver Version: 460.106.00   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 250W |      0MiB / 40536MiB |     31%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805432c1-8cc5-42c2-8da6-3c95473976f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModel\n",
    "from accelerate import Accelerator\n",
    "import time\n",
    "\n",
    "from pydantic import BaseModel, Extra, Field, root_validator\n",
    "from typing import Any, List, Optional, Dict, Sequence\n",
    "from chromadb.utils import embedding_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f060a4-c67b-48a1-bc73-7f57353cfdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cecf3a8-2fff-4dc3-910f-6db44ccea08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_dir = '/gpfs/space/projects/stud_ml_22/NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96371d2e-ea5e-420a-85e8-453a228b9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.schema import Generation\n",
    "from langchain.schema import PromptValue, LLMResult\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# langchain.document_loaders.DataFrameLoader has a quite a limited functionality\n",
    "class DataFrameLoader(BaseLoader):\n",
    "    def __init__(self, data_frame: Any, page_content_columns: List[str]):\n",
    "        if not isinstance(data_frame, pd.DataFrame):\n",
    "            raise ValueError(\n",
    "                f\"Expected data_frame to be a pd.DataFrame, got {type(data_frame)}\"\n",
    "            )\n",
    "        self.data_frame = data_frame\n",
    "        self.page_content_columns = page_content_columns\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        result = []\n",
    "        for i, row in self.data_frame.iterrows():\n",
    "            text = \"\"\n",
    "            metadata = {}\n",
    "            for col in self.page_content_columns:\n",
    "                data = row[col]\n",
    "                if isinstance(data, list):\n",
    "                    text += \"\".join(data) + \"\\n\"\n",
    "                elif isinstance(data, str):\n",
    "                    text += data + \"\\n\"\n",
    "                else:\n",
    "                    print(f\"[IGNORED] [{i}] [{col}] {data}\")\n",
    "\n",
    "            metadata_temp = row.to_dict()\n",
    "            for col in self.page_content_columns:\n",
    "                metadata_temp.pop(col)\n",
    "            # Metadata is a dict where a value can only be str, int, or float. Delete other types.\n",
    "            for key, value in metadata_temp.items():\n",
    "                if isinstance(value, (str, int, float)):\n",
    "                    metadata[key] = value\n",
    "\n",
    "            result.append(Document(page_content=text, metadata=metadata))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "775d3f22-b482-476a-bf6a-fd9a43292e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(encoder_only = False):\n",
    "        PATH_TO_CONVERTED_WEIGHTS = os.path.join(\n",
    "            shared_dir, \"llama/13B_Vicuna_added/\")\n",
    "\n",
    "        device = torch.device(\n",
    "            \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        config = AutoConfig.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
    "        config.max_position_embeddings = 1024\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            PATH_TO_CONVERTED_WEIGHTS,\n",
    "            config=config,\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=not args.no_gradient_checkpointing,\n",
    "            load_in_8bit=True,\n",
    "            device_map={\"\": Accelerator().process_index},\n",
    "            # device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da61bb46-b496-4482-8439-682aa8302ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseMessage, LLMResult, PromptValue, get_buffer_string\n",
    "\n",
    "class LlamaWrapperModel(BaseLanguageModel):\n",
    "    model: Any\n",
    "    \n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        values['model'] = get_model()\n",
    "        # values['model'] = 'model'\n",
    "        return values\n",
    "    \n",
    "    def predict(self, text: str, *, stop: Optional[Sequence[str]] = None) -> str:\n",
    "        pass\n",
    "    \n",
    "    def predict_messages(\n",
    "        self, messages: List[BaseMessage], *, stop: Optional[Sequence[str]] = None\n",
    "    ) -> BaseMessage:\n",
    "        pass\n",
    "\n",
    "    def generate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                        callbacks: Callbacks = None) -> LLMResult:\n",
    "\n",
    "        device = torch.device(\n",
    "            \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        PATH_TO_CONVERTED_TOKENIZER = os.path.join(\n",
    "            shared_dir, \"llama/13B_converted/\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
    "              \n",
    "        with torch.no_grad():\n",
    "            prompt = prompts[0].text\n",
    "            \n",
    "            print(\"Tokenizing...\")\n",
    "            s = time.time()\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            e1 = time.time()\n",
    "            print(\"Time to tokenize: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e1 - s)))\n",
    "            \n",
    "            max_length = len(inputs.input_ids) + 500\n",
    "            \n",
    "            print(\"Generating...\")\n",
    "            generate_ids = self.model.generate(input_ids=inputs.input_ids.to(\n",
    "                device), max_new_tokens=max_length)  # max_length = max_new_tokens + prompt_length\n",
    "            e2 = time.time()\n",
    "            print(\"Time to generate: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e2 - e1)))\n",
    "            \n",
    "            print(\"Decoding...\")\n",
    "            text_result = tokenizer.batch_decode(\n",
    "                generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            e3 = time.time()\n",
    "            print(\"Time to decode: \", time.strftime(\n",
    "                '%H:%M:%S', time.gmtime(e3 - e2)))\n",
    "\n",
    "        generation = Generation(text=text_result)\n",
    "        result = LLMResult(generations=[[generation]])\n",
    "        return result\n",
    "\n",
    "    async def agenerate_prompt(self, prompts: List[PromptValue], stop: Optional[List[str]] = None,\n",
    "                               callbacks: Callbacks = None) -> LLMResult:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eafb5c1-7e66-45fb-8467-4f0460e14ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructorEmbeddings(Embeddings):\n",
    "   \n",
    "    def __init__(self):       \n",
    "        self.model = embedding_functions.InstructorEmbeddingFunction(model_name=\"hkunlp/instructor-base\", device=\"cuda\")\n",
    "       \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.model(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07479bb1-d597-4159-b53e-a60b8d191a65",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b18015-4ffe-4d18-9361-92151a29edea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{shared_dir}/data/documents_with_professors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0d311cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    The name of the course is Private Internationa...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10b69336-8f27-4782-b898-77633dcd853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "loader = DataFrameLoader(df, [\"text\"])\n",
    "# chunk size must be 512 because InstructorEmbeddings max_seq_length = 512\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "#embeddings = DistilbertEmbeddings()\n",
    "embeddings = InstructorEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c83fd-09bc-4f44-a29e-ae3f3fad0a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc986488-6d8b-44cc-82a0-227c72fb96f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_documents([texts[0].page_content])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78e7dc2c-f563-4a9a-9318-f9d29bc8324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# this will create the chroma embedding database!!!\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac42fd64-a44f-4ae1-b691-9e560b2a9d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Kairit Sirts teaches Didactic Practice, Natural Language Processing', metadata={}),\n",
       "  0.14056000113487244),\n",
       " (Document(page_content='Piret Kibur teaches Pulmonary Medicine and Thoracic Surgery', metadata={}),\n",
       "  0.24095147848129272),\n",
       " (Document(page_content='Kaire Piirsalu-Kivihall teaches Qualitative Research Methods', metadata={}),\n",
       "  0.24388602375984192),\n",
       " (Document(page_content='Teet Kaur teaches Singing and Vocal Training IV, Main Instrument', metadata={}),\n",
       "  0.24394838511943817)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score(\"Who's Kairit Sirts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25129f53-87d5-486f-a244-4c54553052fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████| 3/3 [00:12<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaWrapperModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3599eb7-7d4d-4902-8412-dcc8a1f1e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Your job is to answer student questions about the university of Tartu. You have to answer based on some information given below.\n",
    "If there is no relevant information, you should tell the student that you do not know the answer.\n",
    "Do not reveal any details of how question answering process works. Do not mention the information given to you.\n",
    "When possible, rephrase the answer so it follows the grammar rules, flows naturally and is easy to understand.\n",
    "Only use the context from the paragraph that is relevant to the question the most. Only answer the question that is asked. Do not add any additional information.\n",
    " \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b229b64f-4736-4eab-a559-06de223ad241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANNOT LOAD FROM DISK BECAUSE OF EMBD DIM INITIALIZATION BUG\n",
    "# db = Chroma(persist_directory=f\"{shared_dir}/data/chroma\", embedding_function=embeddings)  # load from disk \n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chain_type_kwargs = {\"prompt\": llm_prompt}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs)\n",
    "qa.combine_documents_chain.document_prompt = chain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4072eb3-045b-44d3-8047-497f8fbfb2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Time to tokenize:  00:00:00\n",
      "Generating...\n",
      "Time to generate:  00:00:10\n",
      "Decoding...\n",
      "Time to decode:  00:00:00\n"
     ]
    }
   ],
   "source": [
    "query = \"What courses should I take to learn python?\"\n",
    "response = qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "144bc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your job is to answer student questions about the university of Tartu. You have to answer based on some information given below.\n",
      "If there is no relevant information, you should tell the student that you do not know the answer.\n",
      "Do not reveal any details of how question answering process works. Do not mention the information given to you.\n",
      "When possible, rephrase the answer so it follows the grammar rules, flows naturally and is easy to understand.\n",
      "Only use the context from the paragraph that is relevant to the question the most. Only answer the question that is asked. Do not add any additional information.\n",
      " \n",
      "The name of the course is English for Intermediate Learners. Grammar and Vocabulary Course (100% Web-Based), Level B2.1 > B2.2. The code of it is HVLC.01.063.The purpose of the course is  The objective of the course is to develop students' grammar and vocabulary skills at B2 level. By the end of the course, students will have expanded and consolidated vocabulary, improved their knowledge of grammar, and are able to use the vocabulary and language structures covered during the course. They will have improved their knowledge of the following language structures: reported speech, verb patterns, conditionals, articles and quantifiers, relative clauses.The course is 100% web-based. During the course, students will consolidate their grammar and vocabulary skills by doing various types of interactive exercises, quizzes and tasks as well as taking two tests based on the course materials. The language of instruction is English.Language of instruction is English. The course is offered by Department of English Studies. The course is taught in 2023/2024 years. The course is a Regular course. The course is offered for the folowing study levels: bachelor,master,doctoral,bachelor_master. Number of independent work hours is 78.0. Number of credits is 3.0. The course lecturers are: Anett Raup. Prior learning can be recognised for this course. The course is offered by Faculty of Arts and Humanities. The faculty is located in Tartu linn, \n",
      "Tartu linn, Tartumaa \n",
      "EST. In order to pass the course, all the tasks have to be done, assignments submitted and tests passed within the given time frame.. The grading is non-differentiated (pass, fail, not present). The individual work includes Doing all the grammar, vocabulary, reading and listening exercises; passing two tests (with at least 61% correct answers); submitting four short written assignments. The instructions for all tasks are shown on the Moodle page. All the course materials and tasks are web-based in Moodle...\n",
      "\n",
      "The name of the course is Oral and Written Spanish II. The code of it is FLGR.03.264.The purpose of the course is  The aim of the course is to acquire the intermediate/upper intermediate level of Spanish. The course provides students a command of Spanish conversation formulas and grammar at intermediate/upper intermediate level, the knowledge of basic vocabulary and its use in conversation and in written form.The course teaches to express oneself at an intermediate/upper intermediate level, both orally and in written form. The course is based on texts and grammar and lexical exercises. The seminars will be conducted in a practical way.The course assumes an active participation from the students in making dialogues, in expressing their views on the themes treated, it also assumes composition of short texts.Language of instruction is Estonian. The course is offered by Department of Romance Studies. The course is taught in 2022/2023 years. The course is a Regular course. The course is offered for the folowing study levels: bachelor. Number of practice hours is 60. Number of independent work hours is 96.0. Number of credits is 6.0. The course lecturers are: Triin Lõbus. Prior learning can be recognised for this course. The course is offered by Faculty of Arts and Humanities. The faculty is located in Tartu linn, \n",
      "Tartu linn, Tartumaa \n",
      "EST. The graded tasks for the students include test, final written assessment.Hinne kujuneb kontrolltööde (kokku 40 % lõpphindest) ja kirjaliku eksami (60% lõpphindest) koondhindena. Kui sama kontrolltööd on tehtud kaks korda, võetakse arvesse ka esimest hinnet.. The grading is differentiated (A, B, C, D, E, F, not present). The individual work includes Jooksvad kodused ülesanded...\n",
      "\n",
      "Question: What courses should I take to learn python?\n",
      "Helpful Answer: To learn Python, you can take courses such as \"Introduction to Python\" or \"Intermediate Python\". These courses will cover the basics of Python programming and help you develop your skills in writing code and solving problems. You can find these courses offered by various universities or online learning platforms.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f87e18bb904b63d613a1c1ab8287a8eb0edd4bf3cf85a588f299936c6e031cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
